# Transactions Producer

Produces **credit card transaction events** to Apache Kafka. Reads from a CSV of events (generated by `scripts/prepare_data.py`), optionally replays with time spacing, enriches each row with a random **currency** and **original_amt** (derived from a fixed USD amount and a small exchange-rate map), and sends JSON messages to the Kafka topic `transactions`.

## Role in the platform

- **Input**: `credit_card_transactions_events.csv` in this directory (columns include trans_date_trans_time, user_id, merchant, category, amt, is_fraud, unix_time, etc.). The script renames `amt` to `usd_amt` internally.
- **Output**: One JSON message per event to topic `transactions`. Each message includes the original fields plus `currency` and `original_amt` (and no longer `usd_amt`).
- **Checkpointing**: Writes the last processed row index to a checkpoint file (e.g. `checkpoint.txt`) at a configurable interval. On restart, resumes from that index so replays can be paused and continued.

Downstream, the **transactions consumer** (`ingestion/streaming`) consumes from the same topic, logs events, and (when configured) batches them to GCS as Parquet.

## Configuration

| Env var | Default | Description |
|--------|---------|-------------|
| `KAFKA_BROKER` | `kafka:9092` | Kafka bootstrap servers. |
| `KAFKA_TOPIC` | `transactions` | Topic name. |
| `CHECKPOINT_FILE` | (see config) | Path to checkpoint file. |
| `CHECKPOINT_INTERVAL` | (see config) | Save checkpoint every N events. |

Exchange rates and CSV path are set in `src/config.py`.

## Run locally

From this directory:

```bash
uv sync
# Ensure credit_card_transactions_events.csv exists (run scripts/prepare_data.py first)
export KAFKA_BROKER=localhost:9092
uv run python src/event_producer.py
```

## Docker

Built and run via root `docker-compose` as the `transactions_producer` service. Depends on Kafka; ensure the events CSV is present in the build context or mounted so the producer can read it.
